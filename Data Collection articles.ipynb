{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bunny\n"
     ]
    }
   ],
   "source": [
    "cd C:\\\\Users\\\\bunny #### Setting current directory ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Importing numpy and requests packages ###\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "### Requests sent to index page of the given link ###\n",
    "page = requests.get(\"http://mlg.ucd.ie/modules/COMP41680/archive/index.html\")\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a directory to store all the data files of articles ###\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"C:/Users/bunny/data_files\"):\n",
    "    os.makedirs(\"C:/Users/bunny/data_files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cell, we are extracting all the content from the articles using Beautiful soup and storing each article in a different text file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exception handling for connection error ### \n",
    "from requests.exceptions import ConnectionError\n",
    "\n",
    "### Importing Beautiful soup package and using it to get the content of index page parsing through html ###\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "### Finding the a tags in the class main in the content that was extracted using beautiful soup from index page ###\n",
    "article_name_list = soup.find(class_='main')\n",
    "article_name_list_items = article_name_list.find_all('a')\n",
    "\n",
    "### Creating numpy arrays to store links and one final list to store text ###\n",
    "links_array = np.array([])\n",
    "months_array = np.array([])\n",
    "final_list=[]\n",
    "title_list=[]\n",
    "### First for loop is to iterate through the 12 months links i.e; from jan to dec ###\n",
    "for article_name in article_name_list_items:\n",
    "\n",
    "    links = \"http://mlg.ucd.ie/modules/COMP41680/archive/\" + article_name.get('href') ### getting links through href tag ###\n",
    "    links_array = np.append(links_array, links)  ### storing all the first 12 months in an array ###\n",
    "    #print(links)\n",
    "    \n",
    "    ### Sending request to link belonging to each and every month ###\n",
    "    page_months = requests.get(links)\n",
    "    soup_months = BeautifulSoup(page_months.content, 'html.parser') ### Getting contents of the above links ###\n",
    "    \n",
    "    ### Finding the a tags in class main to get href tags and extract links of articles ###\n",
    "    months_list = soup_months.find(class_ ='main')\n",
    "    months_list_items = months_list.find_all('a')\n",
    "    \n",
    "    ### This for loop is to iterate through all the articles in each month\n",
    "    for articles in months_list_items:\n",
    "        month_links = \"http://mlg.ucd.ie/modules/COMP41680/archive/\" + articles.get('href')\n",
    "        months_array = np.append(months_array, month_links) ### storing all the articles in an array ###\n",
    "        #print(month_links)\n",
    "        \n",
    "        ### sending requests to article links ### \n",
    "        page_data = requests.get(month_links)\n",
    "        soup_article = BeautifulSoup(page_data.content, 'html.parser') ### extracting article page content ###\n",
    "        \n",
    "        ### Finding all the 'p' tags in the articles to get content ###\n",
    "        text_iter = soup_article.findAll('p')\n",
    "        \n",
    "        ### Exception Handling to avoid connection error ###\n",
    "        try:\n",
    "            finaltext = \"\"\n",
    "            \n",
    "            ### This for loop is to go through every 'p' tag text ###\n",
    "            for tag in text_iter:\n",
    "                finaltext = finaltext + tag.getText()\n",
    "            final_list.append(finaltext)\n",
    "                ### Adding every word to final text ###\n",
    "                ### Storing every article in a different text file ###\n",
    "            try:\n",
    "                filename = finaltext.split(\" \")[0] + '_'+ strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()).split(\" \")[1].replace(\":\", \"_\")\n",
    "                f = open(os.getcwd()+\"\\\\\"+\"data_files\\\\\"+filename+\".txt\", 'w+')\n",
    "                f.write(finaltext)\n",
    "                f.close()\n",
    "            ### Exception handling to pass these errors\n",
    "            except UnicodeEncodeError:\n",
    "                pass\n",
    "        except ConnectionError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is to get category and title of each article using beautiful soup through html parser and storing them in the CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "### Finding the a tags in the class main in the content that was extracted using beautiful soup from index page ###\n",
    "article_name_list = soup.find(class_='main')\n",
    "article_name_list_items = article_name_list.find_all('a')\n",
    "\n",
    "### Creating numpy arrays to store links and one final list to store text ###\n",
    "final_list_title=[]\n",
    "category_list=[]\n",
    "### First for loop is to iterate through the 12 months links i.e; from jan to dec ###\n",
    "for article_name in article_name_list_items:\n",
    "\n",
    "    links = \"http://mlg.ucd.ie/modules/COMP41680/archive/\" + article_name.get('href')\n",
    "    \n",
    "        ### Sending request to link belonging to each and every month ###\n",
    "    page_months = requests.get(links)\n",
    "    soup_months = BeautifulSoup(page_months.content, 'html.parser') ### Getting contents of the above links ###\n",
    "    \n",
    "    #category is a list of html tags with the category\n",
    "    category=soup_months.find_all('td',class_=\"category\")\n",
    "    for idx, cat in enumerate(category):\n",
    "        cat = str(cat)\n",
    "        #extract the category by extracting <, > characters\n",
    "        temp_string = cat.split(\">\")[1]\n",
    "        index = temp_string.find(\"<\")\n",
    "        final_string = temp_string[0:index]\n",
    "        final_string = str(final_string.replace('\\xa0',\"\"))\n",
    "\n",
    "        if(len(final_string)<13 ):\n",
    "            #replace the category list with proper categories. \n",
    "            category[idx] = final_string    \n",
    "    category_list.append(category)    \n",
    "    \n",
    "    #extract the titles\n",
    "    title_list = soup_months.findAll('a')\n",
    "    final_title_list = []\n",
    "    for idx, title in enumerate(title_list):\n",
    "        if(idx not in list(range(len(title_list)-4, len(title_list)))):\n",
    "            title = str(title)\n",
    "            start_index = title.find(\">\")\n",
    "            end_index = title.find(\"<\", int(start_index) + 1)\n",
    "            extract_string = title[start_index+1:end_index]\n",
    "            #print(extract_string)\n",
    "            final_title_list.append(extract_string)\n",
    "    final_list_title.append(final_title_list)\n",
    "        \n",
    "    #final_title_list\n",
    "#category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is to convert final title and category list of lists to a flat list and convert them to a pandas series dataframe and save them in the csv file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "flat_title_list = [item for sublist in final_list_title for item in sublist]\n",
    "df = pd.DataFrame(flat_title_list)\n",
    "df.to_csv('title.csv', index=False)\n",
    "\n",
    "flat_category_list = [item for sublist in category_list for item in sublist]\n",
    "df = pd.DataFrame(flat_category_list)\n",
    "df.to_csv('category.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to put all the extracted article data in one text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing pandas to convert the final list to a data frame and storing the final list in a .txt file in the given directory ###\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(final_list)\n",
    "df.to_csv('article_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1408"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
